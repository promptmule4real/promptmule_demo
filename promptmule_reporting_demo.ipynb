{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOf7cur71cN5olI/qfbL3i3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/promptmule4real/promptmule_demo/blob/main/promptmule_reporting_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PromptMule API Demo Suite\n",
        "\n",
        "Welcome to the PromptMule API Demo Suite! This code offers you a comprehensive way to test your interactions with the PromptMule API by generating various prompts, analyzing the semantic scores of responses, and compiling detailed reports on the tokens used and saved through caching. Let's walk through what this code does and how you can make the most of it.\n",
        "\n",
        "#### What is this Code?\n",
        "\n",
        "This script facilitates your interaction with the PromptMule API, a platform that offers optimized API requests to different AI models. By using this demo suite, you can:\n",
        "- Authenticate and get a token for API interactions.\n",
        "- Generate or fetch your user-specific API key.\n",
        "- Send a series of prompt requests to the API, with varying semantic similarities and max response counts.\n",
        "- Track and print the detailed logs of each API response (if verbose mode is enabled).\n",
        "- Compile and display a structured report of the prompt responses and cache savings.\n",
        "\n",
        "#### How to Use:\n",
        "\n",
        "1. **Start the Script**: Run the script in a Python environment.\n",
        "2. **Enter User Details**: You'll be prompted to enter your username, password, and the name of your application.\n",
        "3. **Choose Verbose Mode**: If you'd like to see detailed logs for each API call/response, choose \"yes\" for verbose mode. If you prefer a silent mode with minimal output, select \"no\".\n",
        "4. **Provide Cost-saving Strategy**: Input the name of your cost-saving strategy when prompted. This will be used to generate prompts for testing.\n",
        "5. **Review the Reports**: After all the prompts have been sent and responses received, you'll get:\n",
        "    - A detailed report of the prompt responses.\n",
        "    - A summary of your usage.\n",
        "    - A report on the savings achieved using caching.\n",
        "6. **Profile Deletion (Optional)**: At the end, you'll be given the option to delete your user profile. Only choose this if you're sure, as this action cannot be undone.\n",
        "\n",
        "#### Additional Information:\n",
        "For detailed documentation, further guidance, or any other queries related to the PromptMule API, please visit [PromptMule](https://www.promptmule.com/).\n",
        "\n",
        "Happy testing!"
      ],
      "metadata": {
        "id": "5ngI3FzjrRom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMnLKlvAqu94"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Constants for API interactions\n",
        "ENDPOINT = 'https://api.promptmule.com/'\n",
        "LOGIN = 'login'\n",
        "KEY_GEN = 'api-keys'\n",
        "PROMPT = 'prompt'\n",
        "LIST_API_KEYS = 'api-keys/'\n",
        "DELETE = 'profile/'\n",
        "\n",
        "# User-specific details\n",
        "USERNAME = input(\"Please provide your username: \")\n",
        "PASSWORD = input(\"Please provide your password: \")\n",
        "APPNAME = input(\"Please provide your app name: \")\n",
        "\n",
        "verbose_mode = input(\"Do you want verbose mode enabled (detailed logs)? (yes/no): \").lower() == \"yes\"\n",
        "\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Report Table\n",
        "report_df = pd.DataFrame(columns=['Prompts Sent', 'Tokens Sent', 'Responses Received', 'Tokens Received', 'Cached Responses', 'Tokens Saved by Cache'])\n",
        "\n",
        "# Trackers for the report data\n",
        "prompts_sent = 0\n",
        "tokens_sent = 0\n",
        "responses_received = 0\n",
        "tokens_received = 0\n",
        "cached_responses = 0\n",
        "tokens_saved_by_cache = 0\n",
        "\n",
        "# The rest of the function definitions remain unchanged\n",
        "\n",
        "def send_prompt_requests(cost_saving_strategy, semantic_similarity, max_response_quantity):\n",
        "    global prompts_sent, tokens_sent, responses_received, tokens_received, cached_responses, tokens_saved_by_cache\n",
        "\n",
        "    for variation in range(1, int(prompt_variations * 0.9) + 1):\n",
        "        prompt_text = generate_prompt(cost_saving_strategy, variation)\n",
        "        prompts_sent += 1\n",
        "        tokens_sent += len(prompt_text.split())\n",
        "\n",
        "        api_call_body = {\n",
        "            \"model\": \"gpt-3.5-turbo\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
        "            \"max_tokens\": \"50\",\n",
        "            \"temperature\": \"0.99\",\n",
        "            \"user\": USERNAME,\n",
        "            \"api\": \"openai\",\n",
        "            \"semantic\": str(semantic_similarity),\n",
        "            \"sem_num\": str(max_response_quantity)\n",
        "        }\n",
        "\n",
        "        start_time = time.time()\n",
        "        response = requests.post(ENDPOINT + PROMPT, headers={'x-api-key': api_key, 'Content-Type': 'application/json'}, json=api_call_body)\n",
        "        latency = time.time() - start_time\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            responses_received += 1\n",
        "\n",
        "            choices_data = track_semantic_scores(response.json())\n",
        "            tokens_received += sum([len(choice[2].split()) for choice in choices_data])\n",
        "\n",
        "            # Check if a response is cached\n",
        "            for choice in choices_data:\n",
        "                if \"cached\" in choice and choice[\"cached\"] == True:\n",
        "                    cached_responses += 1\n",
        "                    tokens_saved_by_cache += len(choice[2].split())\n",
        "\n",
        "            if verbose_mode:\n",
        "                print(\"\\nPrompt response:\")\n",
        "                print(json.dumps(response.json(), indent=4))\n",
        "                print(f\"Latency: {latency} seconds\")\n",
        "                print(\"\\nTop Implementation Tip:\", choices_data[0][2])\n",
        "                print(\"\\nOther Tips:\", *[choice[2] for choice in choices_data[1:]], sep=\"\\n\")\n",
        "        else:\n",
        "            print(f\"\\nError with prompt variation {variation}. Status code: {response.status_code}\")\n",
        "            print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "# Main Execution\n",
        "# 1. Log in to the service and update headers with the obtained token\n",
        "token = login_to_promptmule()\n",
        "headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "print(\"Login successful! Token acquired.\")\n",
        "\n",
        "# 2. Generate or retrieve the user's API key\n",
        "api_key = generate_or_fetch_api_key()\n",
        "print(f\"API Key: {api_key}\")\n",
        "\n",
        "# 3. Gather user input for prompt testing\n",
        "cost_saving_strategy = input(\"What do you call your cost-savings strategy while implementing AI applications?: \")\n",
        "\n",
        "# 4. Define constants for prompt variations and requests\n",
        "prompt_variations = 25\n",
        "requests_per_variation = 5\n",
        "df = pd.DataFrame(columns=['Prompt ID', 'Semantic Similarity', 'Score', 'Choice Content', 'Choice Index'])\n",
        "\n",
        "# 5. Send prompt requests with varying semantic similarities and max response counts\n",
        "semantic_nums = [1.0, 0.75, 0.0]\n",
        "max_responses = list(range(1, 11))  # From 1 to 10\n",
        "for sem_num in semantic_nums:\n",
        "    send_prompt_requests(cost_saving_strategy, sem_num, random.choice(max_responses))\n",
        "\n",
        "# 6. Display the results in a structured report\n",
        "print(\"\\nReport:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "def fetch_data(endpoint, headers, success_message, fail_message):\n",
        "    \"\"\"Make a GET request and print the results.\"\"\"\n",
        "    response = requests.get(endpoint, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"\\n{success_message}:\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "    else:\n",
        "        print(f\"\\n{fail_message} {response.status_code}:\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "\n",
        "# 7. Fetch and display user data summary\n",
        "fetch_data(ENDPOINT + LIST_API_KEYS, headers, \"List of API keys\", \"Failed to list API keys\")\n",
        "fetch_data(ENDPOINT + 'usage', {\"Authorization\": f\"Bearer {token}\", \"x-api-key\": api_key}, \"Usage Summary\", \"Failed to fetch usage summary\")\n",
        "fetch_data(ENDPOINT + 'usage/daily-stats', {\"Authorization\": f\"Bearer {token}\", \"x-api-key\": api_key}, \"Daily Usage Stats\", \"Failed to fetch daily usage stats\")\n",
        "\n",
        "print(\"\\nReport Summary:\")\n",
        "print(f\"Username: {USERNAME}\")\n",
        "print(f\"API Key: {api_key}\")\n",
        "print(f\"Number of Cost-saving Strategy Variations Sent: {prompt_variations}\")\n",
        "print(f\"Number of Requests per Strategy Variation: {requests_per_variation}\")\n",
        "\n",
        "# 8. Optionally allow user to delete their profile\n",
        "if input(\"\\nDo you really want to delete your user profile? (yes/no): \").lower() == 'yes':\n",
        "    delete_data(ENDPOINT + DELETE, headers)\n",
        "\n",
        "# Append data to report table\n",
        "report_df.loc[len(report_df)] = [prompts_sent, tokens_sent, responses_received, tokens_received, cached_responses, tokens_saved_by_cache]\n",
        "\n",
        "# Display the report\n",
        "print(\"\\nPromptmule Cache Report:\")\n",
        "print(report_df.to_string(index=False))\n",
        "\n"
      ]
    }
  ]
}
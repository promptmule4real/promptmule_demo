{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNxy/zpMTZH/dGaMgtOWPQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/promptmule4real/promptmule_demo/blob/main/promptmule_reporting_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PromptMule API Demo Suite\n",
        "\n",
        "Welcome to the PromptMule API Demo Suite! This code offers you a comprehensive way to test your interactions with the PromptMule API by generating various prompts, analyzing the semantic scores of responses, and compiling detailed reports on the tokens used and saved through caching. Let's walk through what this code does and how you can make the most of it.\n",
        "\n",
        "#### What is this Code?\n",
        "\n",
        "This script facilitates your interaction with the PromptMule API, a platform that offers optimized API requests to different AI models. By using this demo suite, you can:\n",
        "- Authenticate and get a token for API interactions.\n",
        "- Generate or fetch your user-specific API key.\n",
        "- Send a series of prompt requests to the API, with varying semantic similarities and max response counts.\n",
        "- Track and print the detailed logs of each API response (if verbose mode is enabled).\n",
        "- Compile and display a structured report of the prompt responses and cache savings.\n",
        "\n",
        "#### How to Use:\n",
        "\n",
        "1. **Start the Script**: Run the script in a Python environment.\n",
        "2. **Enter User Details**: You'll be prompted to enter your username, password, and the name of your application.\n",
        "3. **Choose Verbose Mode**: If you'd like to see detailed logs for each API call/response, choose \"yes\" for verbose mode. If you prefer a silent mode with minimal output, select \"no\".\n",
        "4. **Provide Cost-saving Strategy**: Input the name of your cost-saving strategy when prompted. This will be used to generate prompts for testing.\n",
        "5. **Review the Reports**: After all the prompts have been sent and responses received, you'll get:\n",
        "    - A detailed report of the prompt responses.\n",
        "    - A summary of your usage.\n",
        "    - A report on the savings achieved using caching.\n",
        "6. **Profile Deletion (Optional)**: At the end, you'll be given the option to delete your user profile. Only choose this if you're sure, as this action cannot be undone.\n",
        "\n",
        "#### Additional Information:\n",
        "For detailed documentation, further guidance, or any other queries related to the PromptMule API, please visit [PromptMule](https://www.promptmule.com/).\n",
        "\n",
        "Happy testing!"
      ],
      "metadata": {
        "id": "5ngI3FzjrRom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMnLKlvAqu94"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Promptmule Semantic Cache Reporting Guide\n",
        "Date: October 12, 2023\n",
        "Purpose: To provide guidance and best practices for application developers looking to save money\n",
        "during the development and production phases of building generative AI-based applications with OpenAI.\n",
        "\n",
        "Author: the friendly folks at Promptmule\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "\n",
        "# The Endpoints: Constants for API interactions\n",
        "ENDPOINT = 'https://api.promptmule.com/'\n",
        "LOGIN = 'login'\n",
        "KEY_GEN = 'api-keys'\n",
        "PROMPT = 'prompt'\n",
        "LIST_API_KEYS = 'api-keys/'\n",
        "DELETE = 'profile/'\n",
        "\n",
        "# User-specific details\n",
        "# These are what you'll use to define your login and app for Promptmule to know it's you\n",
        "USERNAME = \"YOUR_USER_NAME\"\n",
        "PASSWORD = \"YOUR_PASSWORD\"\n",
        "APPNAME = \"YOUR_APPNAME\"\n",
        "\n",
        "# simple prompt template to give some test prompts for this demo\n",
        "# example: USERS_MESSAGE = \"Please provide a cost-saving strategy for implementing AI applications while we establish the API account: \"\n",
        "#\n",
        "USERS_MESSAGE = \"Please provide a cost-saving strategy for implementing AI applications while we establish the API account: \"\n",
        "\n",
        "# Headers for API interactions\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# --- FUNCTIONS ---\n",
        "\n",
        "def login_to_promptmule():\n",
        "    \"\"\"\n",
        "    Log into PromptMule service and retrieve authentication token.\n",
        "    \"\"\"\n",
        "    data = {\"username\": USERNAME, \"password\": PASSWORD}\n",
        "    response = requests.post(ENDPOINT + LOGIN, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"token\"]\n",
        "    else:\n",
        "        print(f\"Login failed: {response.status_code}\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "        exit()\n",
        "\n",
        "def generate_or_fetch_api_key():\n",
        "    \"\"\"\n",
        "    Attempt to generate a new API key for the user's app or fetch an existing one.\n",
        "    \"\"\"\n",
        "    data = {\"app-name\": APPNAME}\n",
        "    response = requests.post(ENDPOINT + KEY_GEN, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200 and \"api-key\" in response.json():\n",
        "        return response.json()[\"api-key\"]\n",
        "    else:\n",
        "        # If generating a new one fails, try to fetch an existing API key\n",
        "        response = requests.get(ENDPOINT + LIST_API_KEYS, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            for key_data in response.json().get(\"api-keys\", []):\n",
        "                if key_data.get(\"app-name\") == APPNAME:\n",
        "                    return key_data.get(\"api-key\")\n",
        "    print(\"Error fetching or generating API keys.\")\n",
        "    exit()\n",
        "\n",
        "def generate_prompt(users_message, variation):\n",
        "    \"\"\"\n",
        "    Formulate a prompt message given a user's message and a variation number.\n",
        "    \"\"\"\n",
        "    base_prompt = \"You are a developer aiming to save money using OpenAI. Share an implementation tip related to the strategy: \"\n",
        "    return f\"{base_prompt}{users_message}. Variation: {variation}\"\n",
        "\n",
        "def track_semantic_scores(response_json):\n",
        "    \"\"\"\n",
        "    Extract semantic scores and associated data from the API's response.\n",
        "    \"\"\"\n",
        "    return [(choice['prompt-id'], choice.get('score'), choice['message']['content'], choice['index']) for choice in response_json['choices']]\n",
        "\n",
        "def send_prompt_requests(users_message, semantic_similarity, max_response_quantity):\n",
        "    \"\"\"\n",
        "    Send a series of prompt requests to the API based on the specified parameters.\n",
        "    \"\"\"\n",
        "    for variation in range(1, int(prompt_variations * 0.9) + 1):  # Use 90% of the available variations\n",
        "        api_call_body = {\n",
        "            \"model\": \"gpt-3.5-turbo\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": generate_prompt(users_message, variation)}],\n",
        "            \"max_tokens\": \"50\",\n",
        "            \"temperature\": \"0.99\",\n",
        "            \"user\": USERNAME,\n",
        "            \"api\": \"openai\", # this is specific to Promptmule, and selects the LLM you'd like to prompt\n",
        "            \"semantic\": str(semantic_similarity), # this determines the percentage prompt similarity that is acceptable to return from the cache for this call\n",
        "            \"sem_num\": str(max_response_quantity) # this determines the max number of prompt/response pairs to return if they are greater than \"semantic\" percentage match\n",
        "        }\n",
        "\n",
        "        response = requests.post(ENDPOINT + PROMPT, headers={'x-api-key': api_key, 'Content-Type': 'application/json'}, json=api_call_body)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"\\nPrompt response:\")\n",
        "            print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "            choices_data = track_semantic_scores(response.json())\n",
        "            for choice in choices_data:\n",
        "                df.loc[len(df)] = [choice[0], semantic_similarity, choice[1], choice[2], choice[3]]\n",
        "\n",
        "            print(\"\\nTop Implementation Tip:\", choices_data[0][2])\n",
        "            print(\"\\nOther Tips:\", *[choice[2] for choice in choices_data[1:]], sep=\"\\n\")\n",
        "        else:\n",
        "            print(f\"\\nError with prompt variation {variation}. Status code: {response.status_code}\")\n",
        "            print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "def fetch_data(endpoint, headers, success_message, fail_message):\n",
        "    \"\"\"Make a GET request and print the results.\"\"\"\n",
        "    response = requests.get(endpoint, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"\\n{success_message}:\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "    else:\n",
        "        print(f\"\\n{fail_message} {response.status_code}:\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "def delete_data(endpoint, headers):\n",
        "    \"\"\"Make a DELETE request and print the results.\"\"\"\n",
        "    response = requests.delete(endpoint, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(\"\\nOperation successful!\")\n",
        "    else:\n",
        "        print(f\"\\nOperation failed with status code {response.status_code}:\")\n",
        "        print(json.dumps(response.json(), indent=4))\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# 1. Log in to the service and update headers with the obtained token\n",
        "token = login_to_promptmule()\n",
        "headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "print(\"Login successful! Token acquired.\")\n",
        "\n",
        "# 2. Generate or retrieve the user's API key\n",
        "api_key = generate_or_fetch_api_key()\n",
        "print(f\"API Key: {api_key}\")\n",
        "\n",
        "# 3. Gather user input for prompt testing\n",
        "users_message = input(USERS_MESSAGE)\n",
        "\n",
        "# 4. Define constants for prompt variations and requests\n",
        "prompt_variations = 25\n",
        "requests_per_variation = 5\n",
        "df = pd.DataFrame(columns=['Prompt ID', 'Semantic Similarity', 'Score', 'Choice Content', 'Choice Index'])\n",
        "\n",
        "# 5. Send prompt requests with varying semantic similarities and max response counts\n",
        "semantic_nums = [1.0, 0.75, 0.5]\n",
        "max_responses = list(range(1, 11))  # From 1 to 10\n",
        "for sem_num in semantic_nums:\n",
        "    send_prompt_requests(users_message, sem_num, random.choice(max_responses))\n",
        "\n",
        "# 6. Display the results in a structured report\n",
        "print(\"\\nReport:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# 7. Fetch and display user data summary\n",
        "fetch_data(ENDPOINT + LIST_API_KEYS, headers, \"List of API keys\", \"Failed to list API keys\")\n",
        "fetch_data(ENDPOINT + 'usage', {\"Authorization\": f\"Bearer {token}\", \"x-api-key\": api_key}, \"Usage Summary\", \"Failed to fetch usage summary\")\n",
        "fetch_data(ENDPOINT + 'usage/daily-stats', {\"Authorization\": f\"Bearer {token}\", \"x-api-key\": api_key}, \"Daily Usage Stats\", \"Failed to fetch daily usage stats\")\n",
        "\n",
        "print(\"\\nReport Summary:\")\n",
        "print(f\"Username: {USERNAME}\")\n",
        "print(f\"API Key: {api_key}\")\n",
        "print(f\"Number of Cost-saving Strategy Variations Sent: {prompt_variations}\")\n",
        "print(f\"Number of Requests per Strategy Variation: {requests_per_variation}\")\n",
        "\n",
        "# 8. Optionally allow user to delete their profile\n",
        "#if input(\"\\nDo you really want to delete your user profile? (yes/no): \").lower() == 'yes':\n",
        "#    delete_data(ENDPOINT + DELETE, headers)\n"
      ]
    }
  ]
}